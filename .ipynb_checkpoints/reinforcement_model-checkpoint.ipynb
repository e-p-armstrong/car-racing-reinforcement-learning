{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25198824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal here is to build a reinforcement learning skeleton project\n",
    "# that can be adapted both to my own sims and openai gym/other stuff.\n",
    "# Also, there are annotations by nifty bits of code so that I can learn it\n",
    "# better and faster.\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import gym_snake\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# import environment here, or build it in-notebook idc\n",
    "\n",
    "ALPHA = 0.01\n",
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 1000\n",
    "NUM_STEPS_FOR_UPDATE = 5\n",
    "env = gym.make('CarRacing-v2',domain_randomize=True,continuous=False)\n",
    "observation = env.reset()\n",
    "# controller = env.controller()\n",
    "# env.n_foods = 3\n",
    "# env.grid_size = [25,25]\n",
    "# env.unit_size = 10\n",
    "# env.unit_gap = 1\n",
    "# env.snake_size = 1\n",
    "# env.random_init = True\n",
    "#...\n",
    "\n",
    "# When I customize this enough that it's my own, maybe name my setup after Otaku things, because DQN. haha. So funny.\n",
    "# With this, I am basically performing transfer learning on myself: I'm starting with someone else's model (Andrew Ng) and then gradually descending towards\n",
    "# a solution that is more tuned towards me (while improving my knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26414455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "State shape:  (96, 96, 3)\n",
      "Number of actions (Should be 5): 5\n",
      "State and action space full objects: Box(0, 255, (96, 96, 3), uint8) Discrete(5)\n"
     ]
    }
   ],
   "source": [
    "## environment stuff\n",
    "# Note to self: I should make whatever environment I build's API resemble \n",
    "# that of the openai thing, so that I can freely switch between my own\n",
    "# sims and theirs depending on the project.\n",
    "# ADDITIONAL NOTE: minesweeper is probably not a good initial game. Way too luck-based.\n",
    "\n",
    "state_size =  env.observation_space.shape# change based on what is being considered\n",
    "num_actions = env.action_space.n # Figure out how to represent \"actions\" efficiently\n",
    "env.reset()\n",
    "\n",
    "print(env.render())\n",
    "# Image.fromarray(env.render())\n",
    "print(\"State shape: \",env.observation_space.shape)\n",
    "print(\"Number of actions (Should be 5):\",env.action_space.n)\n",
    "print(\"State and action space full objects:\",env.observation_space,env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f7eab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# constants\n",
    "# print(state_size)\n",
    "# dimensions_after_1_conv = convolve_dimensions(state_size[0],3,0,1)\n",
    "# print(\"1conv dimensions\",dimensions_after_1_conv)\n",
    "# dimensions_after_1_pool = pool_dimensions(dimensions_after_1_conv,3)\n",
    "# print(\"1pool dimensions\",dimensions_after_1_pool)\n",
    "\n",
    "# Set up networks\n",
    "q_network = Sequential([\n",
    "    Conv2D(32,3,input_shape=(state_size)),\n",
    "    MaxPooling2D((3,3)),\n",
    "    Conv2D(64,3),\n",
    "    MaxPooling2D((3,3)),\n",
    "    Flatten(),\n",
    "    Dense(units = 50, activation = \"relu\"),\n",
    "    Dense(units = 25, activation = \"relu\"),\n",
    "    Dense(units = num_actions, activation = \"linear\")\n",
    "])\n",
    "\n",
    "# # Set up networks\n",
    "# q_network = Sequential([\n",
    "#     Conv2D(32,3,input_shape=(state_size)),\n",
    "#     MaxPooling2D((3,3)),\n",
    "#     Conv2D(64,dimensions_after_1_pool,dimensions_after_1_pool),\n",
    "#     MaxPooling2D((3,3)),\n",
    "#     Flatten(),\n",
    "#     Dense(units = 64, activation = \"relu\"),\n",
    "#     Dense(units = 32, activation = \"relu\"),\n",
    "#     Dense(units = num_actions)\n",
    "# ])\n",
    "\n",
    "# target_q_network = Sequential([\n",
    "#     Input(state_size),\n",
    "#     Dense(units = 64, activation = \"relu\"),\n",
    "#     Dense(units = 64, activation = \"relu\"),\n",
    "#     Dense(units = num_actions)\n",
    "# ])\n",
    "optimizer = Adam(ALPHA)\n",
    "\n",
    "# define environment tuple\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38dbd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network,):\n",
    "    \n",
    "    # unpack values of tuples as lists (basically take the columns out)\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute Q(s,a) for all experience tuples (this will be used to get the y-target value)\n",
    "    max_q = tf.reduce_max(q_network(next_states), axis=-1)\n",
    "    finished_constant = 1 - done_vals\n",
    "    \n",
    "    y_targets = rewards + gamma*max_qsa*finished_constant\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "    # Get someone clever to tell me what the line below does\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "    \n",
    "    loss = MSE(y_targets,q_values)\n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5d7f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helpers\n",
    "EPSILON_DECAY = 0.99\n",
    "EPSILON_FLOOR = 0.0\n",
    "\n",
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "def update_if_conditions(t,steps_for_update,buffer): # update only every some # of steps\n",
    "                                                     # and also only when buffer is at least as large as the batch size.\n",
    "    if (((steps_for_update % t) == 0) and (len(buffer) > MINIBATCH_SIZE)):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def sample_experiences(buffer): # Taken from Andrew Ng's coursera ML course\n",
    "    experiences = random.sample(buffer,k=MINIBATCH_SIZE)\n",
    "    states = tf.convert_to_tensor( # conversion to tensor so that methods like tf.reduce_max work later\n",
    "    np.array([e.state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    # Note the neat inline for loop and if statement going on. Be sure to ask about that.\n",
    "    actions = tf.convert_to_tensor(\n",
    "        np.array([e.action for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    \n",
    "    rewards = tf.convert_to_tensor(\n",
    "        np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    next_states = tf.convert_to_tensor(\n",
    "        np.array([e.next_state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    done_vals = tf.convert_to_tensor(\n",
    "        np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    return (states, actions, rewards, next_states, done_vals) # all of the above is done to take the tuple columns and make them tensorflow tensors\n",
    "                                                              # so that they're easier to work with. This returns the tensors as a group of separate items.\n",
    "    \n",
    "def update_epsilon(epsilon):\n",
    "    # Reduce epsilon by a fraction during each iteration, to a minimum of\n",
    "    # the epsilon floor\n",
    "    return max(epsilon*EPSILON_DECAY,EPSILON_FLOOR)\n",
    "\n",
    "def get_action_epsilon(qvals,epsilon): # Get lowest-cost action, or pick randomly with epsilon% chance\n",
    "    if (random.random() < epsilon):\n",
    "        # pick random action\n",
    "        choice = random.randint(0,qvals.numpy().shape[0])\n",
    "        return choice # assumes action takes form of a number. # possible alternative from Coursera: return random.choice(np.arange(4))\n",
    "    else:\n",
    "        # pick best action (according to DQN)\n",
    "        choice = np.argmax(qvals.numpy()[0]) # Take the first of the \"max rewards in list\" array generated ny np.argmax. The tensorflow tensor that is qvals needs to be converted first.\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f585d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and apply gradients function\n",
    "# mostly taken from coursera ml spec course\n",
    "@tf.function\n",
    "def agent_learn(experiences,gamma):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences,gamma,q_network)\n",
    "    \n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,q_network.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1857144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = tf.convert_to_tensor(state)\n",
    "state = env.reset()\n",
    "# type(state)\n",
    "# print(state)\n",
    "# type(state[0])\n",
    "# print(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08ad5dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1: total point average over last 200 episodes: 0.0\r",
      "Episode 1: total point average over last 200 episodes: 0.00\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "in user code:\n\n    File \"/var/folders/ky/hw_2k4fn3dvcm9n_kcqzl0xc0000gn/T/ipykernel_1783/1641440239.py\", line 7, in agent_learn  *\n        loss = compute_loss(experiences,gamma,q_network)\n    File \"/var/folders/ky/hw_2k4fn3dvcm9n_kcqzl0xc0000gn/T/ipykernel_1783/481183506.py\", line 10, in compute_loss  *\n        y_targets = rewards + gamma*max_qsa*finished_constant\n\n    NameError: name 'max_qsa' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m sample_experiences(memory_buffer) \u001b[38;5;66;03m#!EA\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# ...And learn from 'em. Set y-targets based on estimates and update\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# DQN weights\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     \u001b[43magent_learn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;66;03m# Since the state is a list, this should be resistant to api changes\u001b[39;00m\n\u001b[1;32m     51\u001b[0m total_points \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/ky/hw_2k4fn3dvcm9n_kcqzl0xc0000gn/T/__autograph_generated_filez7ebj4rw.py:9\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__agent_learn\u001b[0;34m(experiences, gamma)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mFunctionScope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_learn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfscope\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mConversionOptions(recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, user_requested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optional_features\u001b[38;5;241m=\u001b[39m(), internal_convert_user_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;28;01mas\u001b[39;00m fscope:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m----> 9\u001b[0m         loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(compute_loss), (ag__\u001b[38;5;241m.\u001b[39mld(experiences), ag__\u001b[38;5;241m.\u001b[39mld(gamma), ag__\u001b[38;5;241m.\u001b[39mld(q_network)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     10\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(q_network)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(q_network)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/ky/hw_2k4fn3dvcm9n_kcqzl0xc0000gn/T/__autograph_generated_filefln27pmy.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[0;34m(experiences, gamma, q_network)\u001b[0m\n\u001b[1;32m     11\u001b[0m max_q \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_max, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(q_network), (ag__\u001b[38;5;241m.\u001b[39mld(next_states),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)), fscope)\n\u001b[1;32m     12\u001b[0m finished_constant \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(done_vals))\n\u001b[0;32m---> 13\u001b[0m y_targets \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(rewards) \u001b[38;5;241m+\u001b[39m ((ag__\u001b[38;5;241m.\u001b[39mld(gamma) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(max_qsa)) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(finished_constant)))\n\u001b[1;32m     14\u001b[0m q_values \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(q_network), (ag__\u001b[38;5;241m.\u001b[39mld(states),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m q_values \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mgather_nd, (ag__\u001b[38;5;241m.\u001b[39mld(q_values), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mstack, ([ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrange, (ag__\u001b[38;5;241m.\u001b[39mld(q_values)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(actions), ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mint32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)],), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mNameError\u001b[0m: in user code:\n\n    File \"/var/folders/ky/hw_2k4fn3dvcm9n_kcqzl0xc0000gn/T/ipykernel_1783/1641440239.py\", line 7, in agent_learn  *\n        loss = compute_loss(experiences,gamma,q_network)\n    File \"/var/folders/ky/hw_2k4fn3dvcm9n_kcqzl0xc0000gn/T/ipykernel_1783/481183506.py\", line 10, in compute_loss  *\n        y_targets = rewards + gamma*max_qsa*finished_constant\n\n    NameError: name 'max_qsa' is not defined\n"
     ]
    }
   ],
   "source": [
    "# without any of the history stuff to start\n",
    "\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE) # Deques with a max_len remove their first elements\n",
    "                                          # (they're \"pushed out\") if at their max length when new\n",
    "                                          # elements are added to the end. This makes it a natural structure\n",
    "                                          # for a fixed buffer we're drawing from.\n",
    "num_episodes = 2000\n",
    "max_turns = 1000 # Will vary heavily based on what the sim is\n",
    "                 # max-turns here lines up nicely with memory_size\n",
    "stop_at_score = 200\n",
    "\n",
    "total_point_history = []\n",
    "    \n",
    "num_p_av = 200\n",
    "epsilon = 1.0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset(\n",
    "#         options={\"randomize\":True}\n",
    "    )\n",
    "    state = state[0]\n",
    "    total_points = 0 # history stuff\n",
    "    for t in range(max_turns):\n",
    "        # Choose action A (with epsilon-greedy policy) based on current state\n",
    "        state_qn = np.expand_dims(state,axis=0) # State needs to be of expected shape\n",
    "#         print(\"expanded state\", state_qn)\n",
    "#         state_qn = state\n",
    "        q_values = q_network(state_qn)\n",
    "#         print(\"q_values\",q_values)\n",
    "        \n",
    "        action = get_action_epsilon(q_values, epsilon) #!EA make function\n",
    "#         print(env.step(action))\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        memory_buffer.append(experience(state,action,reward,next_state,done))\n",
    "#         print(\"t and numsteps\",t,NUM_STEPS_FOR_UPDATE)\n",
    "        t1 = t+1\n",
    "        update = update_if_conditions(t1,NUM_STEPS_FOR_UPDATE, memory_buffer) #!EA\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples from memory buffer\n",
    "            experiences = sample_experiences(memory_buffer) #!EA\n",
    "            \n",
    "            # ...And learn from 'em. Set y-targets based on estimates and update\n",
    "            # DQN weights\n",
    "            agent_learn(experiences,GAMMA)\n",
    "            \n",
    "        state = next_state.copy() # Since the state is a list, this should be resistant to api changes\n",
    "        total_points += reward\n",
    "#         print(\"made it through a loop!\")\n",
    "        # Keeps iterating continuously in the same situation, unless sim is resolved\n",
    "        if done:\n",
    "            break\n",
    "    total_point_history.append(total_points) # note no assignment here, because .append changes the object which is stored in memory, not some temporary value. No assignment is needed.\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:]) # colon here means \"everything after num_p_av from the left\"\n",
    "    \n",
    "            \n",
    "    epsilon = update_epsilon(epsilon) #!EA make function\n",
    "    \n",
    "    print(f\"\\nEpisode {i+1}: total point average over last {num_p_av} episodes: {av_latest_points}\",end=\"\")\n",
    "    \n",
    "    if ((av_latest_points % num_p_av) == 0):        \n",
    "        # Print permenantly; things after this will be on a new line. This gives some sort of history/context as the algorithm runs.\n",
    "        print(f\"\\rEpisode {i+1}: total point average over last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "        \n",
    "    if av_latest_points >= stop_at_score:\n",
    "        q_network.save('reinforcement_learning.h5')\n",
    "        break\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdf200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get network to make decisions, call network on state and pick best action\n",
    "def decide(state):\n",
    "    expanded = np.expand_dims(state,axis=0) # change state into something tf can work with\n",
    "    decision = q_network(expanded)         # lay out Q values for each choice\n",
    "    return np.argmax(decision.numpy()[0]) # picks choice with highest Q.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b475ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513e101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbc176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5989853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
