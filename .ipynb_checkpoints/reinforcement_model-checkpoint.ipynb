{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25198824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal here is to build a reinforcement learning skeleton project\n",
    "# that can be adapted both to my own sims and openai gym/other stuff.\n",
    "# Also, there are annotations by nifty bits of code so that I can learn it\n",
    "# better and faster.\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "import imageio\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# import environment here, or build it in-notebook idc\n",
    "\n",
    "ALPHA = 0.01\n",
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 1000\n",
    "NUM_STEPS_FOR_UPDATE = 5\n",
    "env = gym.make('CarRacing-v2',domain_randomize=True,continuous=False)\n",
    "observation = env.reset()\n",
    "# controller = env.controller()\n",
    "# env.n_foods = 3\n",
    "# env.grid_size = [25,25]\n",
    "# env.unit_size = 10\n",
    "# env.unit_gap = 1\n",
    "# env.snake_size = 1\n",
    "# env.random_init = True\n",
    "#...\n",
    "\n",
    "# When I customize this enough that it's my own, maybe name my setup after Otaku things, because DQN. haha. So funny.\n",
    "# With this, I am basically performing transfer learning on myself: I'm starting with someone else's model (Andrew Ng) and then gradually descending towards\n",
    "# a solution that is more tuned towards me (while improving my knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26414455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "State shape:  (96, 96, 3)\n",
      "Number of actions (Should be 5): 5\n",
      "State and action space full objects: Box(0, 255, (96, 96, 3), uint8) Discrete(5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/gym/envs/box2d/car_racing.py:569: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CarRacing-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "## environment stuff\n",
    "# Note to self: I should make whatever environment I build's API resemble \n",
    "# that of the openai thing, so that I can freely switch between my own\n",
    "# sims and theirs depending on the project.\n",
    "# ADDITIONAL NOTE: minesweeper is probably not a good initial game. Way too luck-based.\n",
    "\n",
    "state_size =  env.observation_space.shape# change based on what is being considered\n",
    "num_actions = env.action_space.n # Figure out how to represent \"actions\" efficiently\n",
    "env.reset()\n",
    "\n",
    "print(env.render())\n",
    "# Image.fromarray(env.render())\n",
    "print(\"State shape: \",env.observation_space.shape)\n",
    "print(\"Number of actions (Should be 5):\",env.action_space.n)\n",
    "print(\"State and action space full objects:\",env.observation_space,env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f7eab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 21:22:29.693456: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-15 21:22:29.694147: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# constants\n",
    "# print(state_size)\n",
    "# dimensions_after_1_conv = convolve_dimensions(state_size[0],3,0,1)\n",
    "# print(\"1conv dimensions\",dimensions_after_1_conv)\n",
    "# dimensions_after_1_pool = pool_dimensions(dimensions_after_1_conv,3)\n",
    "# print(\"1pool dimensions\",dimensions_after_1_pool)\n",
    "\n",
    "# Set up networks\n",
    "q_network = Sequential([\n",
    "    Conv2D(32,3,input_shape=(state_size)),\n",
    "    MaxPooling2D((3,3)),\n",
    "    Conv2D(64,3),\n",
    "    MaxPooling2D((3,3)),\n",
    "    Flatten(),\n",
    "    Dense(units = 50, activation = \"relu\"),\n",
    "    Dense(units = 25, activation = \"relu\"),\n",
    "    Dense(units = num_actions, activation = \"linear\")\n",
    "])\n",
    "\n",
    "# # Set up networks\n",
    "# q_network = Sequential([\n",
    "#     Conv2D(32,3,input_shape=(state_size)),\n",
    "#     MaxPooling2D((3,3)),\n",
    "#     Conv2D(64,dimensions_after_1_pool,dimensions_after_1_pool),\n",
    "#     MaxPooling2D((3,3)),\n",
    "#     Flatten(),\n",
    "#     Dense(units = 64, activation = \"relu\"),\n",
    "#     Dense(units = 32, activation = \"relu\"),\n",
    "#     Dense(units = num_actions)\n",
    "# ])\n",
    "\n",
    "# target_q_network = Sequential([\n",
    "#     Input(state_size),\n",
    "#     Dense(units = 64, activation = \"relu\"),\n",
    "#     Dense(units = 64, activation = \"relu\"),\n",
    "#     Dense(units = num_actions)\n",
    "# ])\n",
    "optimizer = Adam(ALPHA)\n",
    "\n",
    "# define environment tuple\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38dbd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network,):\n",
    "    \n",
    "    # unpack values of tuples as lists (basically take the columns out)\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute Q(s,a) for all experience tuples (this will be used to get the y-target value)\n",
    "    max_q = tf.reduce_max(q_network(next_states), axis=-1)\n",
    "    finished_constant = 1 - done_vals\n",
    "    \n",
    "    y_targets = rewards + gamma*max_q*finished_constant\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "    # Get someone clever to tell me what the line below does\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "    \n",
    "    loss = MSE(y_targets,q_values)\n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d7f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helpers\n",
    "EPSILON_DECAY = 0.99\n",
    "EPSILON_FLOOR = 0.0\n",
    "\n",
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "def update_if_conditions(t,steps_for_update,buffer): # update only every some # of steps\n",
    "                                                     # and also only when buffer is at least as large as the batch size.\n",
    "    if (((steps_for_update % t) == 0) and (len(buffer) > MINIBATCH_SIZE)):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def sample_experiences(buffer): # Taken from Andrew Ng's coursera ML course\n",
    "    experiences = random.sample(buffer,k=MINIBATCH_SIZE)\n",
    "    states = tf.convert_to_tensor( # conversion to tensor so that methods like tf.reduce_max work later\n",
    "    np.array([e.state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    # Note the neat inline for loop and if statement going on. Be sure to ask about that.\n",
    "    actions = tf.convert_to_tensor(\n",
    "        np.array([e.action for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    \n",
    "    rewards = tf.convert_to_tensor(\n",
    "        np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    next_states = tf.convert_to_tensor(\n",
    "        np.array([e.next_state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    done_vals = tf.convert_to_tensor(\n",
    "        np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    return (states, actions, rewards, next_states, done_vals) # all of the above is done to take the tuple columns and make them tensorflow tensors\n",
    "                                                              # so that they're easier to work with. This returns the tensors as a group of separate items.\n",
    "    \n",
    "def update_epsilon(epsilon):\n",
    "    # Reduce epsilon by a fraction during each iteration, to a minimum of\n",
    "    # the epsilon floor\n",
    "    return max(epsilon*EPSILON_DECAY,EPSILON_FLOOR)\n",
    "\n",
    "def get_action_epsilon(qvals,epsilon): # Get lowest-cost action, or pick randomly with epsilon% chance\n",
    "    if (random.random() < epsilon):\n",
    "        # pick random action\n",
    "        choice = random.randint(0,qvals.numpy().shape[0])\n",
    "        return choice # assumes action takes form of a number. # possible alternative from Coursera: return random.choice(np.arange(4))\n",
    "    else:\n",
    "        # pick best action (according to DQN)\n",
    "        choice = np.argmax(qvals.numpy()[0]) # Take the first of the \"max rewards in list\" array generated ny np.argmax. The tensorflow tensor that is qvals needs to be converted first.\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f585d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and apply gradients function\n",
    "# mostly taken from coursera ml spec course\n",
    "@tf.function\n",
    "def agent_learn(experiences,gamma):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences,gamma,q_network)\n",
    "    \n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,q_network.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1857144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = tf.convert_to_tensor(state)\n",
    "state = env.reset()\n",
    "# type(state)\n",
    "# print(state)\n",
    "# type(state[0])\n",
    "# print(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad5dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250: total point average over last 200 episodes: -19.436710420579687"
     ]
    }
   ],
   "source": [
    "# without any of the history stuff to start\n",
    "\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE) # Deques with a max_len remove their first elements\n",
    "                                          # (they're \"pushed out\") if at their max length when new\n",
    "                                          # elements are added to the end. This makes it a natural structure\n",
    "                                          # for a fixed buffer we're drawing from.\n",
    "num_episodes = 2000\n",
    "max_turns = 1000 # Will vary heavily based on what the sim is\n",
    "                 # max-turns here lines up nicely with memory_size\n",
    "stop_at_score = 2\n",
    "\n",
    "total_point_history = []\n",
    "    \n",
    "num_p_av = 200\n",
    "epsilon = 1.0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset(\n",
    "#         options={\"randomize\":True}\n",
    "    )\n",
    "    state = state[0]\n",
    "    total_points = 0 # history stuff\n",
    "    for t in range(max_turns):\n",
    "        # Choose action A (with epsilon-greedy policy) based on current state\n",
    "        state_qn = np.expand_dims(state,axis=0) # State needs to be of expected shape\n",
    "#         print(\"expanded state\", state_qn)\n",
    "#         state_qn = state\n",
    "        q_values = q_network(state_qn)\n",
    "#         print(\"q_values\",q_values)\n",
    "        \n",
    "        action = get_action_epsilon(q_values, epsilon) #!EA make function\n",
    "#         print(env.step(action))\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        memory_buffer.append(experience(state,action,reward,next_state,done))\n",
    "#         print(\"t and numsteps\",t,NUM_STEPS_FOR_UPDATE)\n",
    "        t1 = t+1\n",
    "        update = update_if_conditions(t1,NUM_STEPS_FOR_UPDATE, memory_buffer) #!EA\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples from memory buffer\n",
    "            experiences = sample_experiences(memory_buffer) #!EA\n",
    "            \n",
    "            # ...And learn from 'em. Set y-targets based on estimates and update\n",
    "            # DQN weights\n",
    "            agent_learn(experiences,GAMMA)\n",
    "            \n",
    "        state = next_state.copy() # Since the state is a list, this should be resistant to api changes\n",
    "        total_points += reward\n",
    "#         print(\"made it through a loop!\")\n",
    "        # Keeps iterating continuously in the same situation, unless sim is resolved\n",
    "        if done:\n",
    "            break\n",
    "    total_point_history.append(total_points) # note no assignment here, because .append changes the object which is stored in memory, not some temporary value. No assignment is needed.\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:]) # colon here means \"everything after num_p_av from the left\"\n",
    "    \n",
    "            \n",
    "    epsilon = update_epsilon(epsilon) #!EA make function\n",
    "    \n",
    "    print(f\"\\rEpisode {i+1}: total point average over last {num_p_av} episodes: {av_latest_points}\",end=\"\")\n",
    "    \n",
    "    if ((i % 10) == 0):        \n",
    "        # Print permenantly; things after this will be on a new line. This gives some sort of history/context as the algorithm runs.\n",
    "        print(f\"\\rEpisode {i+1}: total point average over last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "        \n",
    "    if av_latest_points >= stop_at_score:\n",
    "        q_network.save('reinforcement_learning.h5')\n",
    "        break\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdf200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get network to make decisions, call network on state and pick best action\n",
    "def decide(state):\n",
    "    expanded = np.expand_dims(state,axis=0) # change state into something tf can work with\n",
    "    decision = q_network(expanded)         # lay out Q values for each choice\n",
    "    return np.argmax(decision.numpy()[0]) # picks choice with highest Q.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b475ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make imageio shut up\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "filename = \"./car_racing.mp4\"\n",
    "def create_video(): # Taken from coursera ML specialization by Andrew Ng\n",
    "    with imageio.get_writer(filename, fps=60) as video:\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            state = state[0]\n",
    "            frame = env.render()\n",
    "            video.append_data(frame)\n",
    "            while not done:\n",
    "                state = np.expand_dims(state, axis=0)\n",
    "                q_values = q_network(state)\n",
    "                action = np.argmax(q_values.numpy()[0])\n",
    "                state, _, done, _ = env.step(action)\n",
    "                frame = env.render()\n",
    "                video.append_data(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0513e101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "q_network.save('reinforcement_learning.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73cbc176",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "append_data requires ndarray as first arg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m, in \u001b[0;36mcreate_video\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m state \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m frame \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     14\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/imageio/core/format.py:575\u001b[0m, in \u001b[0;36mFormat.Writer.append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Check image data\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(im, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend_data requires ndarray as first arg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Get total meta dict\u001b[39;00m\n\u001b[1;32m    577\u001b[0m total_meta \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: append_data requires ndarray as first arg"
     ]
    }
   ],
   "source": [
    "create_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5989853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
