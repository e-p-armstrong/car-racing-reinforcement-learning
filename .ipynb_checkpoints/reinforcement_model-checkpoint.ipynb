{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25198824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal here is to build a reinforcement learning skeleton project\n",
    "# that can be adapted both to my own sims and openai gym/other stuff.\n",
    "# Also, there are annotations by nifty bits of code so that I can learn it\n",
    "# better and faster.\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# import environment here, or build it in-notebook idc\n",
    "\n",
    "ALPHA = 0.01\n",
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 1000\n",
    "NUM_STEPS_FOR_UPDATE = 5\n",
    "#...\n",
    "\n",
    "# When I customize this enough, maybe name my setup after Otaku things, because DQN. haha. So funny.\n",
    "# With this, I am basically performing transfer learning on myself: I'm starting with someone else's model (Andrew Ng) and then gradually descending towards\n",
    "# a solution that is more tuned towards me (while improving my knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26414455",
   "metadata": {},
   "outputs": [],
   "source": [
    "## environment stuff\n",
    "# Note to self: I should make whatever environment I build's API resemble \n",
    "# that of the openai thing, so that I can freely switch between my own\n",
    "# sims and theirs depending on the project.\n",
    "# ADDITIONAL NOTE: minesweeper is probably not a good initial game. Way too luck-based.\n",
    "\n",
    "state_size = 100 # change based on what is being considered\n",
    "num_actions = 10 # Figure out how to represent \"actions\" efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f7eab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up networks\n",
    "q_network = Sequential([\n",
    "    Input(state_size),\n",
    "    Dense(units = 128, activation = \"relu\"),\n",
    "    Dense(units = 128, activation = \"relu\"),\n",
    "    Dense(units = num_actions)\n",
    "])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(state_size),\n",
    "    Dense(units = 128, activation = \"relu\"),\n",
    "    Dense(units = 128, activation = \"relu\"),\n",
    "    Dense(units = num_actions)\n",
    "])\n",
    "optimizer = Adam(ALPHA)\n",
    "\n",
    "# define environment tuple\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38dbd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network,):\n",
    "    \n",
    "    # unpack values of tuples as lists (basically take the columns out)\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute Q(s,a) for all experience tuples (this will be used to get the y-target value)\n",
    "    max_q = tf.reduce_max(target_q_network(next_States), axis=-1)\n",
    "    finished_constant = 1 - done_vals\n",
    "    \n",
    "    y_targets = rewards + gamma*max_qsa*finished_constant\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "    # Get someone clever to tell me what the line below does\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "    \n",
    "    loss = MSE(y_targets,q_values)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5d7f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helpers\n",
    "EPSILON_DECAY = 0.99\n",
    "EPSILON_FLOOR = 0.0\n",
    "\n",
    "MINIBATCH_SIZE = 128\n",
    "\n",
    "def update_if_conditions(t,steps_for_update,buffer): # update only every some # of steps\n",
    "                                                     # and also only when buffer is at least as large as the batch size.\n",
    "    if ((steps_for_update % t == 0) and (len(buffer) > MINIBATCH_SIZE)):\n",
    "        return true\n",
    "    return false\n",
    "    \n",
    "def sample_experiences(buffer): # Taken from Andrew Ng's coursera ML course\n",
    "    random.sample(buffer,k=MINIBATCH_SIZE)\n",
    "    states = tf.convert_to_tensor( # conversion to tensor so that methods like tf.reduce_max work later\n",
    "    np.array([e.state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    # Note the neat inline for loop and if statement going on. Be sure to ask about that.\n",
    "    actions = tf.convert_to_tensor(\n",
    "        np.array([e.action for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    \n",
    "    rewards = tf.convert_to_tensor(\n",
    "        np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    next_states = tf.convert_to_tensor(\n",
    "        np.array([e.next_state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    done_vals = tf.convert_to_tensor(\n",
    "        np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    return (states, actions, rewards, next_states, done_vals) # all of the above is done to take the tuple columns and make them tensorflow tensors\n",
    "                                                              # so that they're easier to work with. This returns the tensors as a group of separate items.\n",
    "    \n",
    "def update_epsilon(epsilon):\n",
    "    # Reduce epsilon by a fraction during each iteration, to a minimum of\n",
    "    # the epsilon floor\n",
    "    return max(epsilon*EPSILON_DECAY,EPSILON_FLOOR)\n",
    "\n",
    "def get_action_epsilon(qvals,epsilon): # Get lowest-cost action, or pick randomly with epsilon% chance\n",
    "    if (random.random() > epsilon):\n",
    "        # pick random action\n",
    "        choice = random.randint(0,qvals.numpy().shape[0])\n",
    "        return choice # assumes action takes form of a number. # possible alternative from Coursera: return random.choice(np.arange(4))\n",
    "    else:\n",
    "        # pick best action (according to DQN)\n",
    "        choice = np.argmax(qvals.numpy()[0]) # Take the first of the \"max rewards in list\" array generated ny np.argmax. The tensorflow tensor that is qvals needs to be converted first.\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f585d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and apply gradients function\n",
    "# mostly taken from coursera ml spec course\n",
    "@tf.function\n",
    "def agent_learn(experiences,gamma):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences,gamma,q_network)\n",
    "    \n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients,q_network.trainable_variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ad5dd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m---> 18\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mreset\n\u001b[1;32m     19\u001b[0m     total_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# history stuff\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_turns):\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Choose action A (with epsilon-greedy policy) based on current state\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# without any of the history stuff to start\n",
    "\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE) # Deques with a max_len remove their first elements\n",
    "                                          # (they're \"pushed out\") if at their max length when new\n",
    "                                          # elements are added to the end. This makes it a natural structure\n",
    "                                          # for a fixed buffer we're drawing from.\n",
    "num_episodes = 2000\n",
    "max_turns = 1000 # Will vary heavily based on what the sim is\n",
    "                 # max-turns here lines up nicely with memory_size\n",
    "stop_at_score = 200\n",
    "\n",
    "total_point_history = []\n",
    "    \n",
    "num_p_av = 200\n",
    "epsilon = 1.0\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset\n",
    "    total_points = 0 # history stuff\n",
    "    for t in range(max_turns):\n",
    "        # Choose action A (with epsilon-greedy policy) based on current state\n",
    "        state_qn = np.expand_dims(state, axis=0) # State will be same shape as input, need to do this to get it into 2d array\n",
    "        q_values = q_network(state_qn)\n",
    "        \n",
    "        action = get_action_epsilon(q_values, epsilon) #!EA make function\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        memory_buffer.append(experience(state,action,reward,next_state,done))\n",
    "        \n",
    "        update = update_if_conditions(t,NUM_STEPS_FOR_UPDATE, memory_buffer) #!EA\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples from memory buffer\n",
    "            experiences = sample_experiences(memory_buffer) #!EA\n",
    "            \n",
    "            # ...And learn from 'em. Set y-targets based on estimates and update\n",
    "            # DQN weights\n",
    "            agent_learn(experiences,GAMMA)\n",
    "            \n",
    "        state = next_state.copy() # Since the state is a list, this should be resistant to api changes\n",
    "        total_points += reward\n",
    "        \n",
    "        # Keeps iterating continuously in the same situation, unless sim is resolved\n",
    "        if done:\n",
    "            break\n",
    "    total_point_history.append(total_points) # note no assignment here, because .append changes the object which is stored in memory, not some temporary value. No assignment is needed.\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:]) # colon here means \"everything after num_p_av from the left\"\n",
    "    \n",
    "            \n",
    "    epsilon = update_epsilon(epsilon) #!EA make function\n",
    "    \n",
    "    print(f\"\\nEpisode {i+1}: total point average over last {num_p_av} episodes: {av_lastest_points}\",end=\"\")\n",
    "    \n",
    "    if ((av_latest_points % num_p_av) == 0):        \n",
    "        # Print permenantly; things after this will be on a new line. This gives some sort of history/context as the algorithm runs.\n",
    "        print(f\"\\rEpisode {i+1}: total point average over last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "        \n",
    "    if av_latest_points >= stop_at_score:\n",
    "        q_network.save('reinforcement_learning.h5')\n",
    "        break\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdf200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get network to make decisions, call network on state and pick best action\n",
    "def decide(state):\n",
    "    expanded = np.expand_dims(state,axis=0) # change state into something tf can work with\n",
    "    decision = q_network(expanded)         # lay out Q values for each choice\n",
    "    return np.argmax(decision.numpy()[0]) # picks choice with highest Q.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b475ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513e101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbc176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5989853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
